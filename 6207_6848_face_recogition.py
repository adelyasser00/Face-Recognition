# -*- coding: utf-8 -*-
"""6207_6848_Face_Recogition.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qV6NCu0Qky1ZPKY87H-8EsVKZHFaWlql

# Face recognition

## imports and installs
"""

#@title Unzip required Folder
!unzip archive.zip -d archive

#@title install needed packages
!pip3 install numpy
!pip3 install pillow
!pip3 install scikit-learn
!pip3 install matplotlib

"""## Main part"""

#@title 2) Generate the Data Matrix and the Label vector
import numpy as np
import os
from PIL import Image
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# lists for storing the data matrix D and label vector y
D = []
y = []

# 2) Generate the Data Matrix and the Label vector
for subject in range(1, 41):
    # every subject has 10 images, get 10 images per subject
    imageCount = 0

    for image in os.listdir(f'archive/s{subject}'):
        temp = Image.open(f'archive/s{subject}/{image}')
        vector = np.array(temp).flatten()

        y.append(subject)
        D.append(vector)

# convert the dataMatrix and labels to numpy arrays
D = np.array(D)
y = np.array(y)

#@title 3) Split the data-set into Training and Test sets
num_images = D.shape[0]
rng_idx = np.random.permutation(num_images)
split= 0.5
split_idx = int(num_images * split)
training_data = D[rng_idx[:split_idx]]
testing_data = D[rng_idx[split_idx:]]

training_labels = y[rng_idx[:split_idx]]
testing_labels = y[rng_idx[split_idx:]]

#@title 4) Classification using PCA
# Calculate Projection Matrix U
training_mean = np.mean(training_data, axis=0)
training_std = np.std(training_data, axis=0)
training_centered = training_data - training_mean
covariance_matrix = np.cov(training_centered.T)

testing_mean = np.mean(testing_data, axis=0)
testing_centered = testing_data - testing_mean

"""12 minute operation on T4 GPU"""

#@title continue 4)
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

"""Continue 4)"""

#@title continue 4)
eigenvalues = eigenvalues.real
eigenvectors = eigenvectors.real

# index to sort the eigen values and eigen vectors in decreasing order of eigen values
idx = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[idx]
sorted_eigenvectors = eigenvectors[:, idx]
#to get the variance fraction to choose how many dimension aka how many eigen vectors
total_variance = np.sum(sorted_eigenvalues)

# Compute the cumulative sum of the sorted eigenvalues
cumulative_variance = np.cumsum(sorted_eigenvalues)

# Compute the cumulative proportion of the total variance
cumulative_proportion = cumulative_variance / total_variance

# cumulative_sum = np.cumsum(sorted_eigenvalues)

#@title continue 4) and 5)
# alpha=[0.8,0.85,0.9,0.95]    loop on the array and mark accuracy

alphas = [0.8, 0.85, 0.9, 0.95]  # for example

for alpha in alphas:
  # alpha = 0.8
  # Compute the total variance


  num_eigenvectors = np.where(cumulative_proportion >= alpha)[0][0] + 1
  # final eigen vectors chosen for projection

  projected_eigenvectors = sorted_eigenvectors[:,:num_eigenvectors]

  D_train_pca = training_centered.dot(projected_eigenvectors)
  D_test_pca = testing_centered.dot(projected_eigenvectors)
  # U = sorted_eigenvectors[:, :num_eigenvectors]

  # project all the data on the eigen vectors
  # D_train_pca = np.dot(training_data, U)
  # D_test_pca = np.dot(testing_data, U)

  # training: fitting the points on the graph so the classifier can classify any new testing point
  # 5) Classifier Tuning
  knn_nums = [1, 3, 5, 7]
  accuracies = []
  for knn_num in knn_nums:
      knn = KNeighborsClassifier(n_neighbors=knn_num, weights='distance')
      knn.fit(D_train_pca, training_labels)

      # testing
      predicted_labels = knn.predict(D_test_pca)

      # accuracy
      accuracy = accuracy_score(testing_labels, predicted_labels)
      accuracies.append(accuracy)
      print(f'Accuracy of alpha={alpha}, K={knn_num}: {accuracy}')

  plt.plot(knn_nums, accuracies)
  plt.xlabel('Number of Neighbors (K)')
  plt.ylabel(f'Accuracy of alpha={alpha}')
  plt.title(f'Accuracy vs. Number of Neighbors for alpha={alpha}')
  plt.show()

"""## bonus

### a) 70:30 split
"""

#@title a) 70:30 split
split= 0.7
split_idx = int(num_images * split)
training_data = D[rng_idx[:split_idx]]
testing_data = D[rng_idx[split_idx:]]

training_labels = y[rng_idx[:split_idx]]
testing_labels = y[rng_idx[split_idx:]]

training_mean = np.mean(training_data, axis=0)
training_std = np.std(training_data, axis=0)
training_centered = training_data - training_mean
covariance_matrix = np.cov(training_centered.T)

testing_mean = np.mean(testing_data, axis=0)
testing_centered = testing_data - testing_mean

"""12 minute operation again lol"""

#@title continue 70:30
eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

#@title continue 70:30
eigenvalues = eigenvalues.real
eigenvectors = eigenvectors.real

# index to sort the eigen values and eigen vectors in decreasing order of eigen values
idx = np.argsort(eigenvalues)[::-1]
sorted_eigenvalues = eigenvalues[idx]
sorted_eigenvectors = eigenvectors[:, idx]
#to get the variance fraction to choose how many dimension aka how many eigen vectors
total_variance = np.sum(sorted_eigenvalues)

# Compute the cumulative sum of the sorted eigenvalues
cumulative_variance = np.cumsum(sorted_eigenvalues)

# Compute the cumulative proportion of the total variance
cumulative_proportion = cumulative_variance / total_variance

# cumulative_sum = np.cumsum(sorted_eigenvalues)
# alpha=[0.8,0.85,0.9,0.95]    loop on the array and mark accuracy

alphas = [0.8, 0.85, 0.9, 0.95]  # for example

for alpha in alphas:
  # alpha = 0.8
  # Compute the total variance


  num_eigenvectors = np.where(cumulative_proportion >= alpha)[0][0] + 1
  # final eigen vectors chosen for projection

  projected_eigenvectors = sorted_eigenvectors[:,:num_eigenvectors]

  D_train_pca = training_centered.dot(projected_eigenvectors)
  D_test_pca = testing_centered.dot(projected_eigenvectors)
  # U = sorted_eigenvectors[:, :num_eigenvectors]

  # project all the data on the eigen vectors
  # D_train_pca = np.dot(training_data, U)
  # D_test_pca = np.dot(testing_data, U)

  # training: fitting the points on the graph so the classifier can classify any new testing point
  # 5) Classifier Tuning
  knn_nums = [1, 3, 5, 7]
  accuracies = []
  for knn_num in knn_nums:
      knn = KNeighborsClassifier(n_neighbors=knn_num, weights='distance')
      knn.fit(D_train_pca, training_labels)

      # testing
      predicted_labels = knn.predict(D_test_pca)

      # accuracy
      accuracy = accuracy_score(testing_labels, predicted_labels)
      accuracies.append(accuracy)
      print(f'Accuracy of alpha={alpha}, K={knn_num}: {accuracy}')

  plt.plot(knn_nums, accuracies)
  plt.xlabel('Number of Neighbors (K)')
  plt.ylabel(f'Accuracy of alpha={alpha}')
  plt.title(f'Accuracy vs. Number of Neighbors for alpha={alpha}')
  plt.show()

"""### b) Faces vs non-Faces"""

#@title b) Non face images
# lists for storing the data matrix D and label vector y take the first 200 faces and label them ones
D_truck = []
D_new=[]
y_faces = np.ones(400)
y_trucks = np.zeros(800)
y_new = np.concatenate((y_faces, y_trucks))

#take upto 800 truck images
count=0
for image in os.listdir(f'archive/truck'):
  temp = Image.open(f'archive/truck/{image}')
  # convert to 92*112 and greyscale
  temp = temp.resize((92,112))
  temp = temp.convert('L')
  vector = np.array(temp).flatten()
  D_truck.append(vector)
  count+=1
  if count==800:
    break
D_truck = np.array(D_truck)
D_new = np.concatenate((D , D_truck))

accuracies_iter_max = []
accuracies_iter_min = []
iterations = [200,400,800]
for iteration in iterations:
  print(f'current setting: faces=400, trucks={iteration}')
  D_current = D_new[:(400+iteration)]
  y_current = y_new[:(400+iteration)]
  num_images = D_current.shape[0]
  rng_idx = np.random.permutation(num_images)
  split= 0.5
  split_idx = int(num_images * split)
  training_data = D_current[rng_idx[:split_idx]]
  testing_data = D_current[rng_idx[split_idx:]]

  training_labels = y_current[rng_idx[:split_idx]]
  testing_labels = y_current[rng_idx[split_idx:]]

  training_mean = np.mean(training_data, axis=0)
  training_std = np.std(training_data, axis=0)
  training_centered = training_data - training_mean
  covariance_matrix = np.cov(training_centered.T)

  testing_mean = np.mean(testing_data, axis=0)
  testing_centered = testing_data - testing_mean
  eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)
  eigenvalues = eigenvalues.real
  eigenvectors = eigenvectors.real

  # index to sort the eigen values and eigen vectors in decreasing order of eigen values
  idx = np.argsort(eigenvalues)[::-1]
  sorted_eigenvalues = eigenvalues[idx]
  sorted_eigenvectors = eigenvectors[:, idx]
  #to get the variance fraction to choose how many dimension aka how many eigen vectors
  total_variance = np.sum(sorted_eigenvalues)

  # Compute the cumulative sum of the sorted eigenvalues
  cumulative_variance = np.cumsum(sorted_eigenvalues)

  # Compute the cumulative proportion of the total variance
  cumulative_proportion = cumulative_variance / total_variance
  alphas = [0.8, 0.85, 0.9, 0.95]  # for example

  # a list to store the accuracies for each knn_num
  accuracies_alpha_max = []
  accuracies_alpha_min = []

  for alpha in alphas:
    num_eigenvectors = np.where(cumulative_proportion >= alpha)[0][0] + 1
    # final eigen vectors chosen for projection

    projected_eigenvectors = sorted_eigenvectors[:,:num_eigenvectors]

    D_train_pca = training_centered.dot(projected_eigenvectors)
    D_test_pca = testing_centered.dot(projected_eigenvectors)


    # training: fitting the points on the graph so the classifier can classify any new testing point
    # 5) Classifier Tuning
    knn_nums = [1, 3, 5, 7]
    accuracies_knn = []

    for knn_num in knn_nums:
      print(f'({knn_num}) Nearest Neighbor:')
      success_faces = []
      failure_non_faces = []
      success_non_faces = []
      failure_faces = []
      knn = KNeighborsClassifier(n_neighbors=knn_num, weights='distance')
      knn.fit(D_train_pca, training_labels)

      # testing
      predicted_labels = knn.predict(D_test_pca)
      # Save indices of success and failure cases
      success_faces.extend(np.where((testing_labels == 1) & (predicted_labels == 1))[0])
      failure_non_faces.extend(np.where((testing_labels == 0) & (predicted_labels == 1))[0])
      success_non_faces.extend(np.where((testing_labels == 0) & (predicted_labels == 0))[0])
      failure_faces.extend(np.where((testing_labels == 1) & (predicted_labels == 0))[0])
      # Assuming test_images is your array of test images
      # Assuming y_test are the true labels and y_pred are the predicted labels

      for i in range(3):  # Change this to the number of 2x2 plots you want
        fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10))

        # Success: Faces labeled as faces
        if len(success_faces) > i:
            axes[0, 0].imshow(testing_data[success_faces[i]].reshape(112,92), cmap='gray')
            axes[0, 0].set_title('Success: Faces labeled as faces')
        else:
            axes[0, 0].set_title('No successful face detections')

        # Failure: Non-faces labeled as faces
        if len(failure_non_faces) > i:
            axes[0, 1].imshow(testing_data[failure_non_faces[i]].reshape(112,92), cmap='gray')
            axes[0, 1].set_title('Failure: Non-faces labeled as faces')
        else:
            axes[0, 1].set_title('No mislabeled non-faces')

        # Success: Non-faces labeled as non-faces
        if len(success_non_faces) > i:
            axes[1, 0].imshow(testing_data[success_non_faces[i]].reshape(112,92), cmap='gray')
            axes[1, 0].set_title('Success: Non-faces labeled as non-faces')
        else:
            axes[1, 0].set_title('No successful non-face detections')

        # Failure: Faces labeled as non-faces
        if len(failure_faces) > i:
            axes[1, 1].imshow(testing_data[failure_faces[i]].reshape(112,92), cmap='gray')
            axes[1, 1].set_title('Failure: Faces labeled as non-faces')
        else:
            axes[1, 1].set_title('No mislabeled faces')

        plt.tight_layout()
        plt.show()
      # accuracy
      accuracy = accuracy_score(testing_labels, predicted_labels)
      accuracies_knn.append(accuracy)
      print(f'Accuracy of alpha={alpha}, K={knn_num}: {accuracy}')
    # Append the maximum accuracy among all knn_nums to accuracies_alpha
    accuracies_alpha_max.append(max(accuracies_knn))
    accuracies_alpha_min.append(min(accuracies_knn))

    plt.plot(knn_nums, accuracies_knn)
    plt.xlabel('Number of Neighbors (K)')
    plt.ylabel(f'Accuracy of alpha={alpha}')
    plt.title(f'Accuracy vs. Number of Neighbors for alpha={alpha}')
    plt.show()
  # store performances
  accuracies_iter_max.append(max(accuracies_alpha_max))
  accuracies_iter_min.append(min(accuracies_alpha_min))

plt.plot(iterations, accuracies_iter_max)
plt.plot(iterations, accuracies_iter_min)
plt.xlabel('Number of non-face images')
plt.ylabel('Accuracy')
plt.title('Accuracy vs. Number of Non-face Images')
plt.show()